{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import collections\n",
    "import math\n",
    "import random\n",
    "import pickle\n",
    "from six.moves import xrange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(260391, 8)\n"
     ]
    }
   ],
   "source": [
    "def save2file(filename, data):\n",
    "    pickle_out = open(\"C:/Users/Akarsh/Downloads/DP_scripts/store_emb/\" + filename + \".pickle\", \"wb\")\n",
    "    pickle.dump(data, pickle_out, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    pickle_out.close()\n",
    "\n",
    "def loadfile(filename):\n",
    "    pickle_in = open(\"C:/Users/Akarsh/Downloads/DP_scripts/store_emb/\" + filename + \".pickle\",\"rb\")\n",
    "    return pickle.load(pickle_in)\n",
    "\n",
    "main_str = str(437)\n",
    "\n",
    "filename = \"ip2vec_train_\" + main_str\n",
    "ip2vec_train = loadfile(filename)\n",
    "print(ip2vec_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building dataset...\n",
      "vocab size:  58863\n"
     ]
    }
   ],
   "source": [
    "print(\"building dataset...\")\n",
    "num_elems = -1\n",
    "def build_dataset(ip2vec_train):\n",
    "    global num_elems # keep count of attributes for each flow\n",
    "    data = list(filter(None, ip2vec_train.to_csv(header=False, index=False).splitlines() ))\n",
    "    num_lines = len(data)\n",
    "    num_elems = len(data[0].split(\",\")) # 8 cols\n",
    "    \n",
    "    res = [] #convert to list\n",
    "    for line in data:\n",
    "        for word in line.split(\",\"):\n",
    "            res.append(word.strip())\n",
    "    count = []\n",
    "    count.extend( collections.Counter(res).most_common() ) #count freq    \n",
    "    vocab_size = len(count)\n",
    "    \n",
    "    w2v = dict()\n",
    "    for word, _ in count:\n",
    "        w2v[word] = len(w2v) #w2v\n",
    "    v2w = dict(zip(w2v.values(), w2v.keys())) #v2w\n",
    "    \n",
    "    data = list()\n",
    "    for word in res:\n",
    "        if word in w2v:\n",
    "            index = w2v[word]\n",
    "        data.append(index) #convert to list\n",
    "    return data, num_lines, w2v, v2w, vocab_size\n",
    "\n",
    "data, num_lines, w2v, v2w, vocab_size = build_dataset(ip2vec_train)\n",
    "print(\"vocab size: \", vocab_size)\n",
    "\n",
    "del ip2vec_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "batch_size = 128; embedding_size = 20; num_sampled = 32; num_epochs = 500;\n",
    "data_index = 0; c_iter = 0; pairs = 13; training_pairs = pairs * batch_size; \n",
    "idx = []\n",
    "for i in range(0, num_lines-1):\n",
    "    idx.append(i)\n",
    "\n",
    "def generate_batch():\n",
    "    global data_index; global num_elems; global c_iter\n",
    "    batch = np.ndarray(shape=(training_pairs),dtype=np.int32)\n",
    "    labels= np.ndarray(shape=(training_pairs,1), dtype=np.int32)\n",
    "    data_index = idx[c_iter] * num_elems\n",
    "\n",
    "    for i in range(batch_size):\n",
    "        # input SrcIP       \n",
    "        batch[i*pairs+0] = data[data_index]; labels[i*pairs+0,0] = data[data_index+1]\n",
    "        batch[i*pairs+1] = data[data_index]; labels[i*pairs+1,0] = data[data_index+2]\n",
    "        batch[i*pairs+2] = data[data_index]; labels[i*pairs+2,0] = data[data_index+4]\n",
    "        \n",
    "        # input DstIP\n",
    "        batch[i*pairs+3] = data[data_index+2]; labels[i*pairs+3,0] = data[data_index]\n",
    "        batch[i*pairs+4] = data[data_index+2]; labels[i*pairs+4,0] = data[data_index+4]\n",
    "        batch[i*pairs+5] = data[data_index+2]; labels[i*pairs+5,0] = data[data_index+3]\n",
    "\n",
    "        # input srcPt\n",
    "        batch[i*pairs+6] = data[data_index+1]; labels[i*pairs+6,0] = data[data_index+0]\n",
    "\n",
    "        # input dstPt\n",
    "        batch[i*pairs+7] = data[data_index+3]; labels[i*pairs+7,0] = data[data_index+2]\n",
    "\n",
    "        # input dur\n",
    "        batch[i*pairs+8] = data[data_index+7]; labels[i*pairs+8,0] = data[data_index+5]\n",
    "\n",
    "        # input byt\n",
    "        batch[i*pairs+9] = data[data_index+6]; labels[i*pairs+9,0] = data[data_index+5]\n",
    "        batch[i*pairs+10] = data[data_index+6]; labels[i*pairs+10,0] = data[data_index+7]\n",
    "\n",
    "        # input packets\n",
    "        batch[i*pairs+11] = data[data_index+5]; labels[i*pairs+11,0] = data[data_index+6]\n",
    "        batch[i*pairs+12] = data[data_index+5]; labels[i*pairs+12,0] = data[data_index+7]\n",
    "\n",
    "        # Check if end of training list is reached\n",
    "        c_iter += 1\n",
    "        if c_iter == num_lines - 1:\n",
    "            c_iter = 0\n",
    "            random.shuffle(idx)\n",
    "        data_index = idx[c_iter] * num_elems\n",
    "    return batch, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tensorflow graph...\n",
      "WARNING:tensorflow:From c:\\users\\akarsh\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow_core\\python\\ops\\nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    }
   ],
   "source": [
    "print(\"building tensorflow graph...\")\n",
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default(): \n",
    "    train_inputs = tf.placeholder(tf.int32,shape=[training_pairs])\n",
    "    train_labels = tf.placeholder(tf.int32,shape=[training_pairs,1])\n",
    "\n",
    "    with tf.device('/cpu:0'): \n",
    "        embeddings = tf.Variable(tf.random_uniform([vocab_size,embedding_size],-1.0,1.0))\n",
    "        embed = tf.nn.embedding_lookup(embeddings, train_inputs)\n",
    "\n",
    "        nce_weights = tf.Variable(tf.truncated_normal([vocab_size, embedding_size],stddev=1.0 / math.sqrt(embedding_size)))\n",
    "        nce_biases = tf.Variable(tf.zeros([vocab_size]))\n",
    "        loss = tf.reduce_mean(tf.nn.nce_loss(weights=nce_weights, biases=nce_biases, labels=train_labels, \n",
    "                                             inputs=embed, num_sampled=num_sampled, num_classes=vocab_size))\n",
    "\n",
    "        optimizer = tf.train.GradientDescentOptimizer(0.05).minimize(loss)\n",
    "        \n",
    "        norm = tf.sqrt(tf.reduce_sum(tf.square(embeddings), 1, keepdims=True))\n",
    "        normalized_embeddings = embeddings / norm\n",
    "        init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training steps:  1017152\n",
      "Average loss at step 0 :  136.0756378173828  from  1017152  steps.\n",
      "Average loss at step 20000 :  65.17678718366624  from  1017152  steps.\n",
      "Average loss at step 40000 :  31.80580021824837  from  1017152  steps.\n",
      "Average loss at step 60000 :  24.438414533948897  from  1017152  steps.\n",
      "Average loss at step 80000 :  20.697956525397302  from  1017152  steps.\n",
      "Average loss at step 100000 :  18.39078493566513  from  1017152  steps.\n",
      "Average loss at step 120000 :  16.716911771726608  from  1017152  steps.\n",
      "Average loss at step 140000 :  15.440608703804017  from  1017152  steps.\n",
      "Average loss at step 160000 :  14.35392348074913  from  1017152  steps.\n",
      "Average loss at step 180000 :  13.487041939735413  from  1017152  steps.\n",
      "Average loss at step 200000 :  12.711474601078033  from  1017152  steps.\n",
      "Average loss at step 220000 :  12.034106461453439  from  1017152  steps.\n",
      "Average loss at step 240000 :  11.467714803314209  from  1017152  steps.\n",
      "Average loss at step 260000 :  10.93618140989542  from  1017152  steps.\n",
      "Average loss at step 280000 :  10.471246483039856  from  1017152  steps.\n",
      "Average loss at step 300000 :  10.04537866821289  from  1017152  steps.\n",
      "Average loss at step 320000 :  9.641207266807557  from  1017152  steps.\n",
      "Average loss at step 340000 :  9.267481783461571  from  1017152  steps.\n",
      "Average loss at step 360000 :  8.935798380458355  from  1017152  steps.\n",
      "Average loss at step 380000 :  8.626125493395328  from  1017152  steps.\n",
      "Average loss at step 400000 :  8.33065444148779  from  1017152  steps.\n",
      "Average loss at step 420000 :  8.058527435839176  from  1017152  steps.\n",
      "Average loss at step 440000 :  7.797327355146408  from  1017152  steps.\n",
      "Average loss at step 460000 :  7.536250376856327  from  1017152  steps.\n",
      "Average loss at step 480000 :  7.325394168043137  from  1017152  steps.\n",
      "Average loss at step 500000 :  7.091665639805794  from  1017152  steps.\n",
      "Average loss at step 520000 :  6.910714583826065  from  1017152  steps.\n",
      "Average loss at step 540000 :  6.686589097034931  from  1017152  steps.\n",
      "Average loss at step 560000 :  6.510779084455967  from  1017152  steps.\n",
      "Average loss at step 580000 :  6.338736701834202  from  1017152  steps.\n",
      "Average loss at step 600000 :  6.174508960807324  from  1017152  steps.\n",
      "Average loss at step 620000 :  6.023463654267788  from  1017152  steps.\n",
      "Average loss at step 640000 :  5.861391306769848  from  1017152  steps.\n",
      "Average loss at step 660000 :  5.73738139371872  from  1017152  steps.\n",
      "Average loss at step 680000 :  5.5868146494269375  from  1017152  steps.\n",
      "Average loss at step 700000 :  5.449091994011402  from  1017152  steps.\n",
      "Average loss at step 720000 :  5.329044437742233  from  1017152  steps.\n",
      "Average loss at step 740000 :  5.226450390315056  from  1017152  steps.\n",
      "Average loss at step 760000 :  5.099973392915726  from  1017152  steps.\n",
      "Average loss at step 780000 :  4.995773428499699  from  1017152  steps.\n",
      "Average loss at step 800000 :  4.884665967214108  from  1017152  steps.\n",
      "Average loss at step 820000 :  4.800424027967453  from  1017152  steps.\n",
      "Average loss at step 840000 :  4.699012767481804  from  1017152  steps.\n",
      "Average loss at step 860000 :  4.615717838048935  from  1017152  steps.\n",
      "Average loss at step 880000 :  4.525945510149002  from  1017152  steps.\n",
      "Average loss at step 900000 :  4.4489508401632305  from  1017152  steps.\n",
      "Average loss at step 920000 :  4.366001319897175  from  1017152  steps.\n",
      "Average loss at step 940000 :  4.303115403497219  from  1017152  steps.\n",
      "Average loss at step 960000 :  4.2257833191871645  from  1017152  steps.\n",
      "Average loss at step 980000 :  4.161654608201981  from  1017152  steps.\n",
      "Average loss at step 1000000 :  4.099180201721191  from  1017152  steps.\n",
      "training finished... \n",
      "\n",
      "saving embeddings...\n"
     ]
    }
   ],
   "source": [
    "num_steps = int(num_lines / batch_size * num_epochs)\n",
    "print(\"training steps: \", num_steps)\n",
    "\n",
    "with tf.Session(graph=graph) as session: \n",
    "    init.run()\n",
    "    average_loss = 0\n",
    "    for step in xrange(num_steps): \n",
    "        batch_inputs, batch_labels = generate_batch()\n",
    "        feed_dict = {train_inputs: batch_inputs, train_labels: batch_labels}\n",
    "        _, loss_val = session.run([optimizer, loss], feed_dict=feed_dict)\n",
    "        average_loss += loss_val\n",
    "\n",
    "        if step % 20000 == 0: \n",
    "            if step > 0:\n",
    "                average_loss /= 20000\n",
    "            print(\"Average loss at step\", step, \": \", average_loss, \" from \", num_steps ,\" steps.\")\n",
    "            average_loss = 0\n",
    "    print('training finished...', '\\n')\n",
    "    \n",
    "    # Save embeddings\n",
    "    to_save_n = session.run(embeddings) \n",
    "    to_save = (to_save_n - to_save_n.min(0)) / to_save_n.ptp(0) \n",
    "    to_save_norm = to_save / to_save.max(axis=0)\n",
    "    \n",
    "    final_emb = pd.DataFrame(data=to_save_norm[0:, 0:])\n",
    "    vals = []\n",
    "    for u in range(0,len(to_save)):\n",
    "        vals.append(v2w.get(u))\n",
    "    final_emb['values'] = vals\n",
    "    print('saving embeddings...')\n",
    "    \n",
    "    filename = \"ip2vec_emb_\" + main_str\n",
    "    save2file(filename, final_emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>values</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.374719</td>\n",
       "      <td>0.242699</td>\n",
       "      <td>0.804956</td>\n",
       "      <td>0.233550</td>\n",
       "      <td>0.753552</td>\n",
       "      <td>0.161305</td>\n",
       "      <td>0.899202</td>\n",
       "      <td>0.471473</td>\n",
       "      <td>0.639400</td>\n",
       "      <td>0.082626</td>\n",
       "      <td>...</td>\n",
       "      <td>0.585408</td>\n",
       "      <td>0.393872</td>\n",
       "      <td>0.928697</td>\n",
       "      <td>0.461868</td>\n",
       "      <td>0.389630</td>\n",
       "      <td>0.020627</td>\n",
       "      <td>0.514851</td>\n",
       "      <td>0.879726</td>\n",
       "      <td>0.615675</td>\n",
       "      <td>TCP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.690727</td>\n",
       "      <td>0.289209</td>\n",
       "      <td>0.624222</td>\n",
       "      <td>0.787160</td>\n",
       "      <td>0.259145</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.844552</td>\n",
       "      <td>0.869673</td>\n",
       "      <td>0.398222</td>\n",
       "      <td>0.751319</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.432327</td>\n",
       "      <td>0.184115</td>\n",
       "      <td>0.275347</td>\n",
       "      <td>0.720820</td>\n",
       "      <td>0.338340</td>\n",
       "      <td>0.552738</td>\n",
       "      <td>0.503326</td>\n",
       "      <td>0.304589</td>\n",
       "      <td>0.0_d</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.700514</td>\n",
       "      <td>0.424836</td>\n",
       "      <td>0.185812</td>\n",
       "      <td>0.616382</td>\n",
       "      <td>0.469560</td>\n",
       "      <td>0.492044</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.564201</td>\n",
       "      <td>0.792898</td>\n",
       "      <td>0.874603</td>\n",
       "      <td>...</td>\n",
       "      <td>0.329597</td>\n",
       "      <td>0.540815</td>\n",
       "      <td>0.640496</td>\n",
       "      <td>0.986693</td>\n",
       "      <td>0.467639</td>\n",
       "      <td>0.614137</td>\n",
       "      <td>0.588136</td>\n",
       "      <td>0.802519</td>\n",
       "      <td>0.292566</td>\n",
       "      <td>1_k</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.229812</td>\n",
       "      <td>0.721760</td>\n",
       "      <td>0.847710</td>\n",
       "      <td>0.425830</td>\n",
       "      <td>0.227527</td>\n",
       "      <td>0.530943</td>\n",
       "      <td>0.329691</td>\n",
       "      <td>0.724129</td>\n",
       "      <td>0.800538</td>\n",
       "      <td>...</td>\n",
       "      <td>0.757808</td>\n",
       "      <td>0.089902</td>\n",
       "      <td>0.220831</td>\n",
       "      <td>0.921574</td>\n",
       "      <td>0.741282</td>\n",
       "      <td>0.982664</td>\n",
       "      <td>0.848599</td>\n",
       "      <td>0.535690</td>\n",
       "      <td>0.367750</td>\n",
       "      <td>443_p</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.197396</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.483233</td>\n",
       "      <td>0.585403</td>\n",
       "      <td>0.172215</td>\n",
       "      <td>0.472147</td>\n",
       "      <td>0.425286</td>\n",
       "      <td>0.326074</td>\n",
       "      <td>0.365241</td>\n",
       "      <td>0.838361</td>\n",
       "      <td>...</td>\n",
       "      <td>0.956589</td>\n",
       "      <td>0.395134</td>\n",
       "      <td>0.096285</td>\n",
       "      <td>0.809280</td>\n",
       "      <td>0.605448</td>\n",
       "      <td>0.637512</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.906251</td>\n",
       "      <td>0.830612</td>\n",
       "      <td>80_p</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4         5         6  \\\n",
       "0  0.374719  0.242699  0.804956  0.233550  0.753552  0.161305  0.899202   \n",
       "1  0.690727  0.289209  0.624222  0.787160  0.259145  1.000000  0.844552   \n",
       "2  0.700514  0.424836  0.185812  0.616382  0.469560  0.492044  0.000000   \n",
       "3  0.000000  0.229812  0.721760  0.847710  0.425830  0.227527  0.530943   \n",
       "4  0.197396  0.000000  0.483233  0.585403  0.172215  0.472147  0.425286   \n",
       "\n",
       "          7         8         9  ...        11        12        13        14  \\\n",
       "0  0.471473  0.639400  0.082626  ...  0.585408  0.393872  0.928697  0.461868   \n",
       "1  0.869673  0.398222  0.751319  ...  1.000000  0.432327  0.184115  0.275347   \n",
       "2  0.564201  0.792898  0.874603  ...  0.329597  0.540815  0.640496  0.986693   \n",
       "3  0.329691  0.724129  0.800538  ...  0.757808  0.089902  0.220831  0.921574   \n",
       "4  0.326074  0.365241  0.838361  ...  0.956589  0.395134  0.096285  0.809280   \n",
       "\n",
       "         15        16        17        18        19  values  \n",
       "0  0.389630  0.020627  0.514851  0.879726  0.615675     TCP  \n",
       "1  0.720820  0.338340  0.552738  0.503326  0.304589   0.0_d  \n",
       "2  0.467639  0.614137  0.588136  0.802519  0.292566     1_k  \n",
       "3  0.741282  0.982664  0.848599  0.535690  0.367750   443_p  \n",
       "4  0.605448  0.637512  1.000000  0.906251  0.830612    80_p  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(58863, 21)\n"
     ]
    }
   ],
   "source": [
    "filename = \"ip2vec_emb_\" + main_str\n",
    "ip2vec_emb = loadfile(filename)\n",
    "\n",
    "display(ip2vec_emb.head())\n",
    "print(ip2vec_emb.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data and hyperparams saved...\n"
     ]
    }
   ],
   "source": [
    "save = True\n",
    "if save:\n",
    "    df = pd.DataFrame.from_records([{'operation': 'ip2vec_train', 'main_str': main_str, 'vocab_size': vocab_size,\n",
    "                                     'batch_size': batch_size, 'embedding_size': embedding_size, \n",
    "                                     'negative_samp': num_sampled, 'lr': 0.05, 'num_epochs': num_epochs, 'final loss': average_loss}])\n",
    "    \n",
    "    df.to_csv(\"C:/Users/Akarsh/Downloads/DP_scripts/store_emb/store_params.csv\", mode='a', index=False)\n",
    "    print('data and hyperparams saved...')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
